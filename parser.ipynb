{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPjU4j7QTOcR",
        "outputId": "521969c7-531d-46af-cb7f-4458cef4b726"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Elo9Cq_AUjOM"
      },
      "outputs": [],
      "source": [
        "DRIVE_ROOT = r'/content/drive/My Drive/Colab Notebooks/'\n",
        "DATA_DIR = DRIVE_ROOT + 'data'\n",
        "# PIP_DIR = DRIVE_ROOT + 'pip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcWS48JLZsQi"
      },
      "outputs": [],
      "source": [
        "# remove old files in DATADIR\n",
        "!find \"/content/drive/My Drive/Colab Notebooks/data/\" -type f \\( -iname \\*.jpg -o -iname \\*.xlsx \\) -newermt \"May 13 23:00\" -delete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdmRO97Ztipy",
        "outputId": "7e5f5aa0-ae1a-4461-d6f1-450a74383f69"
      },
      "outputs": [],
      "source": [
        "%pip -q install -U numpy pandas validators yt_dlp easyocr spacy python-Levenshtein thefuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgtAbNo3SAqO",
        "outputId": "e5196929-378e-401c-f888-20333b22a602"
      },
      "outputs": [],
      "source": [
        "# install SpaCy Russian lang model (comment after first use!)\n",
        "\n",
        "# spacywhl = PIP_DIR + '/ru_core_news_md-3.3.0-py3-none-any.whl'\n",
        "# %pip install --upgrade --target=\"$PIP_DIR\" \"$spacywhl\"\n",
        "# !python -m spacy download ru_core_news_md\n",
        "# %pip -q install --upgrade \"$spacywhl\"\n",
        "%pip -q install -U https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.3.0/ru_core_news_md-3.3.0-py3-none-any.whl\n",
        "%pip -q install -U https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.3.0/xx_ent_wiki_sm-3.3.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idOjKOfscpK4"
      },
      "outputs": [],
      "source": [
        "# add PIP dir to PATH in first place to search in that dir first\n",
        "# sys.path.insert(0, PIP_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWYbBMtKAf1s"
      },
      "outputs": [],
      "source": [
        "# uninstall old (built-in) Spacy\n",
        "\n",
        "# !pip uninstall -y spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvLtPtOzPLVC"
      },
      "outputs": [],
      "source": [
        "import os, sys, cv2, validators, yt_dlp, datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import easyocr\n",
        "import spacy\n",
        "import ru_core_news_md\n",
        "from spacy.lang.xx import MultiLanguage\n",
        "from thefuzz import fuzz\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "import traceback\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIcpURVG8mQV",
        "outputId": "7e288051-51a0-4943-c932-3254a4c627a5"
      },
      "outputs": [],
      "source": [
        "spacy.prefer_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CymY2jDzPLVJ"
      },
      "outputs": [],
      "source": [
        "def showimg(img):\n",
        "    dpi = matplotlib.rcParams['figure.dpi']\n",
        "    height, width, _ = img.shape\n",
        "    figsize = width / float(dpi), height / float(dpi)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc1yK24Q6kmm"
      },
      "outputs": [],
      "source": [
        "class ParserException(RuntimeError):\n",
        "    pass\n",
        "\n",
        "class EmptyResultsException(ParserException):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B7VHfq22jIA"
      },
      "outputs": [],
      "source": [
        "class Parser:\n",
        "    \"\"\"\n",
        "    Класс для распознавания текста на серии изображений и выдачи результата\n",
        "    в виде единого списка пар \"тег - значение\", где \"тег\" - это категория / класс,\n",
        "    а \"значение\" - имя/фамилия человека. \n",
        "\n",
        "    Класс использует EasyOCR для нахождения текстовых блоков и распознавания текста,\n",
        "    далее сортирует блоки по строкам и столбцам, определяет, в каких блоках\n",
        "    содержатся имена/фамилии людей (при помощи SpaCy), и выдает список\n",
        "    пар \"тег - ФИО\". Также сами выделенные блоки текста и исходные изображения\n",
        "    можно обрабатывать в callback-процедуре (например, для формирования данных\n",
        "    для собственной модели распознавания текста).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, images=None, langs=[\"ru\", \"en\"], on_parse=None,\n",
        "                 optimize_duplicates=85,\n",
        "                 min_confidence=0, mark_images=True, \n",
        "                 contrast_ths=0.4, adjust_contrast=0.7, \n",
        "                 width_ths=0.55, add_margin=0.02, mag_ratio=1.7, **kwargs):\n",
        "        self.images = images\n",
        "        self.min_confidence = min_confidence\n",
        "        self.mark_images = mark_images\n",
        "        self.on_parse = on_parse\n",
        "        self.optimize_duplicates = optimize_duplicates\n",
        "        self.ocr_kwargs = kwargs if kwargs else {}\n",
        "        self.ocr_kwargs.update(dict(contrast_ths=contrast_ths, adjust_contrast=adjust_contrast, \n",
        "                                    width_ths=width_ths, add_margin=add_margin, mag_ratio=mag_ratio))\n",
        "        self.ocr_engine = easyocr.Reader([l for l in langs] if langs else ['ru', 'en'], gpu=True, \n",
        "                                         # recog_network='armfilm', # custom OCR neural network\n",
        "                                         model_storage_directory=DATA_DIR, user_network_directory=DATA_DIR)\n",
        "        if 'ru' in langs:\n",
        "            self.nlp = ru_core_news_md.load() # Russian model (medium)\n",
        "        else:\n",
        "            self.nlp = MultiLanguage()\n",
        "        \n",
        "    def parse(self):\n",
        "        \"\"\"\n",
        "        Основной метод класса - разбирает исходные изображения один за другим\n",
        "        и выводит сформированный список пар \"тег - ФИО\".\n",
        "        \"\"\"\n",
        "        if not self.images:\n",
        "            raise ParserException('Нет изображений для обработки!')\n",
        "        parsed = None\n",
        "        imgs = []\n",
        "        cnt_images = len(self.images)\n",
        "        for i, imgfile in enumerate(self.images):\n",
        "            try:\n",
        "                blocks, img = self.ocr(imgfile, self.min_confidence, True, self.mark_images, **self.ocr_kwargs)\n",
        "                imgs.append(img)\n",
        "                parsed = self.parse_names(blocks, parsed)\n",
        "                if self.on_parse:\n",
        "                    self.on_parse(i, cnt_images, imgfile, img, blocks, parsed)\n",
        "            except EmptyResultsException:\n",
        "                print(f'Изображение {i+1} / {cnt_images}: текстовые данные не найдены!')\n",
        "                continue\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "        if self.optimize_duplicates >= 0 and self.optimize_duplicates < 100:\n",
        "            parsed, opt_cnt = self.optimize_parsed_names(parsed, self.optimize_duplicates)\n",
        "            if opt_cnt:\n",
        "                print(f'ИСКЛЮЧЕНО {opt_cnt} ДУБЛИКАТОВ.')\n",
        "        return (self.parsed_to_df(parsed), imgs)\n",
        "\n",
        "    def make_training_dataset(self, dataset_name, save_dir=DATA_DIR, images=None):\n",
        "        \"\"\"\n",
        "        Создает датасет из блоков текста (изображений) и таблицы соответствия файл / текст\n",
        "        для возможности дальнейшего обучения собственной модели OCR, которая может\n",
        "        скармливаться EasyOCR.\n",
        "\n",
        "        Описание процесса - https://github.com/JaidedAI/EasyOCR/blob/master/custom_model.md\n",
        "        \"\"\"\n",
        "        if images is None:\n",
        "            images = self.images\n",
        "        if images is None:\n",
        "            raise ParserException('Нет изображений для обработки!')\n",
        "\n",
        "        dspath = os.path.join(save_dir, dataset_name)\n",
        "        if not os.path.exists(dspath):\n",
        "            # создать папку, если ее нет\n",
        "            os.makedirs(dspath)\n",
        "        else:\n",
        "            # удалить все файлы\n",
        "            for f in glob.glob(os.path.join(dspath, '*')):\n",
        "                try:\n",
        "                    os.remove(f)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        pairs = []\n",
        "        counter = 0\n",
        "        for imgfile in images:\n",
        "            try:\n",
        "                blocks, img = self.ocr(imgfile, self.min_confidence, False, False, **self.ocr_kwargs)\n",
        "                for _, row in blocks.iterrows():\n",
        "                    outfile = f'{counter:05}_{dataset_name}.jpg'\n",
        "                    cropped = img[row.tl_y:row.bl_y, row.tl_x:row.tr_x]\n",
        "                    cv2.imwrite(os.path.join(dspath, outfile), cropped)\n",
        "                    pairs.append((outfile, row.text))\n",
        "                    counter += 1\n",
        "            except EmptyResultsException:\n",
        "                print(f'Изображение \"{imgfile}\": текстовые данные не найдены!')\n",
        "                continue\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        if not pairs:\n",
        "            return (None, dspath)\n",
        "\n",
        "        dfpairs = pd.DataFrame.from_records(pairs, columns=['filename', 'words'])\n",
        "        dfpairs.to_csv(os.path.join(dspath, 'labels.csv'), index=False)\n",
        "        return (dfpairs, dspath)\n",
        "\n",
        "    def parsed_to_df(self, parsed):\n",
        "        \"\"\"\n",
        "        Преобразует список пар \"тег - значение\" в DataFrame.\n",
        "        \"\"\"\n",
        "        if not parsed:\n",
        "            return None\n",
        "        dfparsed = pd.DataFrame.from_records(parsed, columns=['tag', 'name']).drop_duplicates()\n",
        "        return dfparsed\n",
        "\n",
        "    def ocr_results_to_dataframe(self, results):\n",
        "        \"\"\"\n",
        "        Преобразует список результатов OCR в DataFrame.\n",
        "        Описание столбцов - см. https://www.jaided.ai/easyocr/tutorial/\n",
        "        \"\"\"\n",
        "        if not results:\n",
        "            return None\n",
        "        flat_list = [[el for l1 in row[0] for el in l1] + list(row[1:]) for row in results]\n",
        "        df = pd.DataFrame.from_records(flat_list, columns=['tl_x', 'tl_y', 'tr_x', 'tr_y', 'br_x', 'br_y', 'bl_x', 'bl_y', 'text', 'conf'])\n",
        "        df.insert(8, 'ht', df['bl_y'] - df['tl_y'])\n",
        "        df.insert(9, 'wd', df['tr_x'] - df['tl_x'])\n",
        "        dtype = {c: np.int16 for c in df.columns[:10]}\n",
        "        dtype.update({'text': str, 'conf': float})\n",
        "        df = df.astype(dtype)\n",
        "        df['text'] = df['text'].str.strip()\n",
        "        return df\n",
        "\n",
        "    def ocr(self, img, min_confidence=0, detect_names=True, mark_image=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Выполняет OCR (распознавание текста) на данном изображении,\n",
        "        формируя на выходе DataFrame с блоками текста и их координатами / размерами.\n",
        "        Блоки сортируются по положению: сверху вниз и слева направо.\n",
        "\n",
        "        kwargs - see https://www.jaided.ai/easyocr/documentation/\n",
        "        \"\"\"\n",
        "        if isinstance(img, str):\n",
        "            img = cv2.imread(img)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        if not kwargs:\n",
        "            kwargs = {}\n",
        "        \n",
        "        results = self.ocr_results_to_dataframe(self.ocr_engine.readtext(img, **kwargs))\n",
        "        if results is None:\n",
        "            raise EmptyResultsException()\n",
        "        \n",
        "        if min_confidence:\n",
        "            results = results.loc[results['conf'] >= min_confidence].copy()\n",
        "\n",
        "        if results is None:\n",
        "            raise EmptyResultsException()\n",
        "\n",
        "        lres = len(results)\n",
        "        results['tag'] = [''] * lres\n",
        "\n",
        "        # убрать лишние блоки\n",
        "        results = self.clean_blocks_heur(results)\n",
        "        if results is None:\n",
        "            raise EmptyResultsException()        \n",
        "\n",
        "        # объединить соседние блоки\n",
        "        results = self.chain_blocks_heur(results)\n",
        "\n",
        "        # кластеризация по \"строкам\", чтобы читать сверху вниз и слева направо\n",
        "        results = self.cluster_blocks_heur(results)\n",
        "\n",
        "        if detect_names:\n",
        "            # определить тип текста: фамилия / имя или нечто другое\n",
        "            lres = len(results)          \n",
        "            for i in range(lres):\n",
        "                txt = results.iat[i, 10]\n",
        "                while '  ' in txt:\n",
        "                    txt = txt.replace('  ', ' ')\n",
        "                spl = txt.split(' ')\n",
        "                if len(spl) > 1 and all(c[0] == c[0].upper() for c in spl if c):\n",
        "                    results.iat[i, 12] = 'name'\n",
        "                    continue\n",
        "                doc = self.nlp(txt)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ == 'PER':\n",
        "                        results.iat[i, 12] = 'name'\n",
        "                        break\n",
        "   \n",
        "        img1 = img.copy()\n",
        "        if mark_image:     \n",
        "            img_shapes = img1.copy()\n",
        "            alpha = 0.5\n",
        "            for row in results.itertuples(False):\n",
        "                color = (230, 230, 50) if row.tag == 'name' else (230, 50, 50)\n",
        "                cv2.rectangle(img_shapes, (row.tl_x, row.tl_y), (row.br_x, row.br_y), color, -1)\n",
        "            cv2.addWeighted(img_shapes, alpha, img1, 1 - alpha, 0, img1)\n",
        "\n",
        "        return (results, img1)\n",
        "\n",
        "    def clean_blocks_heur(self, blocks):\n",
        "        \"\"\"\n",
        "        Удаление лишних блоков.\n",
        "        \"\"\"\n",
        "        # Убираем блоки, не содержащие букв, а также длиной менее 2 символов\n",
        "        blocks = blocks.loc[(blocks.text.str.contains(r'\\w', False)) & (blocks.text.str.len() > 2)]\n",
        "        if len(blocks) == 0: return None\n",
        "        # Вычисляем наиболее частую высоту блока (строки) и удаляем блоки, превышающие 1.5 значения\n",
        "        mode_ht = blocks.ht.mode().iat[0]\n",
        "        blocks = blocks.loc[blocks.ht <= (1.5 * mode_ht)]\n",
        "\n",
        "        return blocks.copy()\n",
        "\n",
        "    def chain_blocks_heur(self, blocks, horizontal=0.5):\n",
        "        \"\"\"\n",
        "        Сцепляет вместе рядом стоящие однородные блоки, которые не были объединены автоматически.\n",
        "        \"\"\"\n",
        "        mode_ht = blocks.ht.mode().iat[0]\n",
        "        cnt = len(blocks)\n",
        "        # горизонтальное сцепление              \n",
        "        max_dist = round(mode_ht * horizontal)\n",
        "        i = 0\n",
        "        while i < cnt:\n",
        "            if blocks.iat[i, -1] != 'DEL':\n",
        "                j = 0\n",
        "                while j < cnt:                    \n",
        "                    if all([\n",
        "                        (i != j), # выбрать другой блок\n",
        "                        (blocks.iat[j, 0] - blocks.iat[i, 2] > 0), # мин. расстояние по горизонтали между tr_x[1] и tl_x[2]\n",
        "                        (blocks.iat[j, 0] - blocks.iat[i, 2] <= max_dist), # макс. расстояние по горизонтали между tr_x[1] и tl_x[2]\n",
        "                        (abs(blocks.iat[j, 1] - blocks.iat[i, 1]) <= max_dist) # макс. расстояние по вертикали между tl_y[1] и tl_y[2]\n",
        "                        ]):\n",
        "                        # tr_x, tr_y, br_x, br_y\n",
        "                        blocks.iloc[i, 2:6] = blocks.iloc[j, 2:6].copy()\n",
        "                        # ht\n",
        "                        blocks.iat[i, 8] = blocks.iat[i, 7] - blocks.iat[i, 1]\n",
        "                        # wd\n",
        "                        blocks.iat[i, 9] = blocks.iat[i, 2] - blocks.iat[i, 0]\n",
        "                        # text\n",
        "                        blocks.iat[i, 10] = ' '.join(blocks.iloc[[i, j], 10].to_list())\n",
        "                        # conf\n",
        "                        blocks.iat[i, 11] = blocks.iloc[[i, j], 11].mean()\n",
        "                        # label - отмечаем старый блок для удаления (уже объединили)\n",
        "                        blocks.iat[j, -1] = 'DEL'\n",
        "                        i -= 1\n",
        "                        break\n",
        "                    j += 1\n",
        "            i += 1\n",
        "        blocks = blocks.loc[blocks.tag != 'DEL']\n",
        "\n",
        "        return blocks.copy()\n",
        "\n",
        "    def split_blocks_heur(self, blocks, vertical=2.0, horizontal=3.0):\n",
        "        \"\"\"\n",
        "        Разбивает исходные данные (блоки) на группы блоков, разделенные по\n",
        "        минимальному расстоянию по вертикали и/или горизонтали.\n",
        "        Возвращает список датафреймов, отсортированный по координатам (tl_x, tl_y).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def cluster_blocks_heur(self, blocks, vertical=0.25, horizontal=0.2):\n",
        "        \"\"\"\n",
        "        Выделение строк и столбцов среди блоков, определение выравнивания\n",
        "        для дальнейшего решения о том, с какой стороны теги (роли), с какой - имена.\n",
        "        \"\"\"\n",
        "        # обычная высота блока\n",
        "        mode_ht = blocks.ht.mode().iat[0]\n",
        "        cnt = len(blocks)\n",
        "\n",
        "        # новый столбец для номеров строк\n",
        "        blocks['row'] = np.full(len(blocks), -1, dtype=np.int16)\n",
        "\n",
        "        # макс. отклонение по вертикали для поиска блоков в строке (25% от высоты блока)\n",
        "        max_dist = round(mode_ht * vertical)\n",
        "\n",
        "        # вычисляем номера строк для всех блоков\n",
        "        i = 0\n",
        "        c = -1\n",
        "        while i < cnt:\n",
        "            # пропускаем блоки с уже присвоенным номером строки    \n",
        "            if blocks.iat[i, -1] == -1:\n",
        "                c += 1\n",
        "                j = 0\n",
        "                while j < cnt:\n",
        "                    # если встретили блок, чей нижний край отклоняется не более max_dist...\n",
        "                    if (blocks.iat[j, -1] == -1) and (abs(blocks.iat[j, 5] - blocks.iat[i, 5]) <= max_dist):\n",
        "                        # присваиваем номер строки\n",
        "                        blocks.iat[j, -1] = c\n",
        "                    j += 1                \n",
        "            i += 1\n",
        "\n",
        "        # теперь у нас есть блоки по строкам:\n",
        "        # 0: [__________]   [_____]    [______]\n",
        "        # 1:      [_________________]  [___]\n",
        "        # 2:             [______]\n",
        "        \n",
        "        last_row = blocks.row.max()\n",
        "        # сортируем данные по строкам и координате X (лев. верх. вершина)\n",
        "        blocks.sort_values(['row', 'tl_x'], inplace=True)\n",
        "        # а теперича создадим столбец для хранения позиции в строке (слева направо)\n",
        "        blocks['hindex'] = np.full(len(blocks), -1, dtype=np.int16)\n",
        "        # вычисляем позицию каждого блока в каждой строке - цикл по номерам строк\n",
        "        for r in range(last_row + 1):\n",
        "            # все блоки в данной строке\n",
        "            l = len(blocks.loc[blocks.row == r])\n",
        "            # просто присваиваем им номера по порядку начиная с 0\n",
        "            blocks.loc[blocks['row'] == r, 'hindex'] = list(range(l))\n",
        "\n",
        "        # теперь у нас есть позции блоков в каждой строке:\n",
        "        # 0: [0_________]   [1____]    [2_____]\n",
        "        # 1:      [0________________]  [1__]\n",
        "        # 2:             [0_____]\n",
        "\n",
        "        # сортируем по строкам и столбцам\n",
        "        blocks.sort_values(['row', 'hindex'], inplace=True)\n",
        "        \n",
        "        return blocks.copy()\n",
        "\n",
        "    def split_names(self, txt):\n",
        "        \"\"\"\n",
        "        Служебный метод для разбиения строк, разделенных запятой.\n",
        "        \"\"\"\n",
        "        spl = [t.strip() for t in txt.split(',')]\n",
        "        return spl if len(spl) > 1 else spl[0]\n",
        "\n",
        "    def parse_names(self, blocks, parsed=None):\n",
        "        \"\"\"\n",
        "        После операции кластеризации (cluster_blocks_heur) проходит по строкам\n",
        "        и выписывает все пары тег-имя в список.\n",
        "        \"\"\"\n",
        "        if parsed is None:\n",
        "            # список извлеченных пар тег-имя в формате: \n",
        "            # [ [tag, name], [tag, name], ... ]\n",
        "            parsed = []\n",
        "\n",
        "        # все блоки уже сортированы по порядку (сверху внизу и слева направо):\n",
        "        # [1_______]   [2________]\n",
        "        #     [3____________]\n",
        "        #    [4_____]  [5____] [6_______]\n",
        "\n",
        "        # применим простое правило: сверху/слева - тег, справа/снизу - имя\n",
        "        tags = []\n",
        "        for _, drow in blocks.iterrows():\n",
        "            # если наши блок, определенный как ФИО\n",
        "            if drow.tag == 'name':\n",
        "                # если есть накопленные выше теги\n",
        "                if tags:\n",
        "                    # объединяем теги через пробел и добавляем ФИО\n",
        "                    parsed.append([' '.join(tags), drow.text])\n",
        "                    tags.clear()\n",
        "                # тегов нет, надо посмотреть последний в списке\n",
        "                else:\n",
        "                    tag = None\n",
        "                    # если список уже содержит пары тег-имя\n",
        "                    if parsed:\n",
        "                        # ищем первый тег в обратном порядке\n",
        "                        for t, _ in parsed[::-1]:\n",
        "                            if t:\n",
        "                                tag = t\n",
        "                                break\n",
        "                    # если тег не нашли, будет None\n",
        "                    parsed.append([tag, drow.text])\n",
        "            else:\n",
        "                tags.append(drow.text)       \n",
        "\n",
        "        return parsed\n",
        "\n",
        "    def optimize_parsed_names(self, parsed, match_cutoff=85):\n",
        "        \"\"\"\n",
        "        Оптимизация финальной таблицы с тегами и именами:\n",
        "        - удаление дублирующихся пар\n",
        "        \"\"\"\n",
        "        if (match_cutoff is None) or (match_cutoff < 0) or (match_cutoff > 100):\n",
        "            return (parsed, 0)\n",
        "        parsed1 = [el + [True] for el in parsed]\n",
        "        lp = len(parsed1)\n",
        "        for i in range(lp):\n",
        "            if not parsed1[i][-1]: continue\n",
        "            for j in range(lp):\n",
        "                if i == j or not parsed1[j][-1]: continue\n",
        "                if fuzz.ratio(''.join(parsed1[i][:-1]), ''.join(parsed1[j][:-1])) > match_cutoff:\n",
        "                    parsed1[j][-1] = False\n",
        "        parsed2 = [el[:-1] for el in parsed1 if el[-1]]\n",
        "        opt_count = lp - len(parsed2)\n",
        "        return (parsed2, opt_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_rNGnn5PLVK"
      },
      "outputs": [],
      "source": [
        "class Extractor:\n",
        "\n",
        "    def isurl(s):\n",
        "        res = validators.url(s.strip())\n",
        "        try:\n",
        "            return res\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def __init__(self, uri, datadir, parser=None, unique_id=None, \n",
        "                 on_download=None, on_extract=None, on_parse=None):\n",
        "        self.videofile = uri\n",
        "        self.datadir = datadir        \n",
        "        self.id = unique_id if not unique_id is None else 'JOB__' + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        self.on_download = on_download \n",
        "        self.on_extract = on_extract\n",
        "        self.parser = parser or Parser(on_parse=on_parse)\n",
        "        self._get_file()\n",
        "        \n",
        "        self.videocap = cv2.VideoCapture(self.videofile)\n",
        "        self.get_video_properties()\n",
        "\n",
        "    def _get_file(self):\n",
        "        if os.path.isfile(self.videofile):\n",
        "            return\n",
        "        elif Extractor.isurl(self.videofile):\n",
        "            try:\n",
        "                fname = self.download(self.videofile, self.datadir)\n",
        "                if fname:\n",
        "                    self.videofile = fname\n",
        "                else:\n",
        "                    raise Exception(f'Unable to download URL {self.videofile} to file!')\n",
        "            except Exception as err:\n",
        "                raise ParserException(str(err))\n",
        "        else:\n",
        "            raise ParserException(f'URI {self.videofile} is not an existing file or a valid URL!')\n",
        "\n",
        "    def download(self, url, savedir):\n",
        "        def dhook(d):\n",
        "            if not self.on_download:\n",
        "                return\n",
        "            if d['status'] != 'error':\n",
        "                self.on_download(d['status'], d['filename'], d.get('downloaded_bytes', 0), d.get('total_bytes', 0), d.get('elapsed', 0), d.get('eta', 0))\n",
        "            else:\n",
        "                self.on_download(d['status'], d['filename'])\n",
        "        ydl_opts = {'format': '(webm/mp4/flv/3gp)/bestvideo[height<=?1080]/best',\n",
        "                    'progress_hooks': [dhook], 'outtmpl': os.path.join(savedir, '%(title)s.%(ext)s')}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, True)\n",
        "            filename = ydl.prepare_filename(info)\n",
        "        try:\n",
        "            return filename\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def get_video_properties(self):\n",
        "        self.frame_count = int(self.videocap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        self.fps = self.videocap.get(cv2.CAP_PROP_FPS)\n",
        "        self.frame_height = int(self.videocap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        self.frame_width = int(self.videocap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        self.duration = self.frame_count / self.fps\n",
        "\n",
        "    def extract_frames(self, time_start=None, time_end=None, sample_interval=2.0, on_get_frame=None):\n",
        "        if time_start:\n",
        "            self.videocap.set(cv2.CAP_PROP_POS_MSEC, time_start * 1000)\n",
        "        if time_end:\n",
        "            time_end *= 1000\n",
        "        else:\n",
        "            time_end = self.duration * 1000\n",
        "        sample_interval *= 1000\n",
        "\n",
        "        i = 0\n",
        "        ok, img = self.videocap.read()\n",
        "        while ok:\n",
        "            i += 1\n",
        "            cur_time = self.videocap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "            cur_frame = self.videocap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "            if on_get_frame:\n",
        "                on_get_frame(img, i, cur_time, cur_frame)\n",
        "\n",
        "            new_time = cur_time + sample_interval\n",
        "            if new_time > time_end:\n",
        "                break\n",
        "            self.videocap.set(cv2.CAP_PROP_POS_MSEC, new_time)\n",
        "            ok, img = self.videocap.read() \n",
        "\n",
        "        # get last frame\n",
        "        self.videocap.set(cv2.CAP_PROP_POS_MSEC, time_end - self.fps)\n",
        "        ok, img = self.videocap.read() \n",
        "        if ok:\n",
        "            cur_time = self.videocap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "            cur_frame = self.videocap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "            if on_get_frame:\n",
        "                on_get_frame(img, i + 1, cur_time, cur_frame)\n",
        "\n",
        "    def extract_and_save_frames(self, time_start=None, time_end=None, sample_interval=2.0):\n",
        "        images = []\n",
        "        def callback(img, n, cur_time, cur_frame):\n",
        "            nonlocal images\n",
        "            imfile = os.path.join(self.datadir, f'{self.id}_{n:05}_frame_{cur_frame:.0f}_{cur_time:.0f}ms.jpg')\n",
        "            cv2.imwrite(imfile, img)\n",
        "            # print(f'EXTRACTED FRAME {n} ==> \"{imfile}\"')\n",
        "            if self.on_extract:\n",
        "                self.on_extract(img, n, cur_time, cur_frame)\n",
        "            images.append(imfile)\n",
        "        self.extract_frames(time_start, time_end, sample_interval, callback)\n",
        "        return images\n",
        "\n",
        "    def extract_and_get_frames(self, time_start=None, time_end=None, sample_interval=2.0):\n",
        "        images = []\n",
        "        def callback(img, n, cur_time, cur_frame):\n",
        "            nonlocal images\n",
        "            # print(f'EXTRACTED FRAME {n}')\n",
        "            if self.on_extract:\n",
        "                self.on_extract(img, n, cur_time, cur_frame)\n",
        "            images.append(img)\n",
        "        self.extract_frames(time_start, time_end, sample_interval, callback)\n",
        "        return images\n",
        "\n",
        "    def extract_and_parse(self, images=None, time_start=None, time_end=None, sample_interval=2.0, **kwargs):\n",
        "        if not images:\n",
        "            images = self.extract_and_save_frames(time_start, time_end, sample_interval)\n",
        "        if not images:\n",
        "            raise ParserException('Не удалось извлечь фреймы из видео!')\n",
        "        self.parser.images = images\n",
        "        if kwargs:\n",
        "            self.parser.ocr_kwargs.update(kwargs)\n",
        "        return self.parser.parse() # dfparsed, imgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajFzHeAQS3ki"
      },
      "outputs": [],
      "source": [
        "files = [\n",
        "# 'JOB__2022-05-13_15-23-22_00001_frame_176_7000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00002_frame_276_11000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00003_frame_376_15000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00004_frame_476_19000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00005_frame_576_23000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00006_frame_676_27000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00007_frame_776_31000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00008_frame_876_35000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00009_frame_976_39000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00010_frame_1076_43000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00011_frame_1176_47000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00012_frame_1276_51000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00013_frame_1376_55000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00014_frame_1476_59000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00015_frame_1576_63000ms.jpg',\n",
        "'JOB__2022-05-13_15-23-22_00016_frame_1676_67000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00017_frame_1776_71000ms.jpg',\n",
        "'JOB__2022-05-13_15-23-22_00018_frame_1876_75000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00019_frame_1976_79000ms.jpg',\n",
        "'JOB__2022-05-13_15-23-22_00020_frame_2076_83000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00021_frame_2176_87000ms.jpg',\n",
        "'JOB__2022-05-13_15-23-22_00022_frame_2276_91000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00023_frame_2376_95000ms.jpg',\n",
        "# 'JOB__2022-05-13_15-23-22_00024_frame_2398_0ms.jpg'\n",
        "]\n",
        "files = [f'{DATA_DIR}/{f}' for f in files]\n",
        "\n",
        "utube_1 = 'https://www.youtube.com/watch?v=ZqSRAPa6QP0' # ru (старый фильм)\n",
        "utube_2 = 'https://www.youtube.com/watch?v=VwTfEU-p0Os' # ru (Лунтик)\n",
        "utube_3 = 'https://www.youtube.com/watch?v=AoOEzu4Lx10' # en (Мстители)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBTRdnawXYy9"
      },
      "outputs": [],
      "source": [
        "def on_parse(i, cnt_images, imgfile, img, blocks, parsed):\n",
        "    # blocks.to_excel(os.path.splitext(imgfile)[0] + '.xlsx', index=False)\n",
        "    print(f'PARSED {i+1} / {cnt_images}, extracted {len(blocks)} blocks.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvCJrf_meY1e"
      },
      "outputs": [],
      "source": [
        "def make_train_dataset(dsname, images, langs=['ru', 'en']):\n",
        "    parser = Parser(images, langs)\n",
        "    dfpairs, dsdir = parser.make_training_dataset(dsname)\n",
        "    print(dfpairs.head().to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smkj9K0Kevdv"
      },
      "outputs": [],
      "source": [
        "def extract1():\n",
        "    parser = Parser(None, ['ru'], on_parse=on_parse)\n",
        "    extractor = Extractor(DATA_DIR + '/mmm.mp4', DATA_DIR, parser)\n",
        "    dfparsed, imgs = extractor.extract_and_parse(files, 7.0, None, 4.0)\n",
        "    return (dfparsed, imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mklOX_8FfGur"
      },
      "outputs": [],
      "source": [
        "def extract2():\n",
        "    parser = Parser(None, ['ru'], on_parse=on_parse)\n",
        "    extractor = Extractor(utube_2, DATA_DIR, parser)\n",
        "    dfparsed, imgs = extractor.extract_and_parse(time_start=1, time_end=None, sample_interval=5.0)\n",
        "    return (dfparsed, imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT20TdlvgFoN",
        "outputId": "c2b0300d-48d4-4709-8ae3-708051cec70d"
      },
      "outputs": [],
      "source": [
        "dfparsed, imgs = extract1()\n",
        "dfparsed.to_excel(os.path.join(DATA_DIR, 'parsed.xlsx'), index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "parser.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0e5f94a1af76b7d291031aef2a0fef594467fb9af05b7b1c0f50f6051df74fcc"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit (system)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
